\chapter{Introduction}

% TODO
% \begin{flushright}
% \rightskip=.8cm\textit{``TODO''} \\
% \vspace{.2em}
% \rightskip=.8cm TODO
% \end{flushright}
% \vspace{1em}

% « Le vent se lève!...
% il faut tenter de vivre »
% 	(IT)
% 
% « Si alza il vento!...
% bisogna tentare di vivere »
% (Paul Valéry, Le cimetière marin[5][6])

Since ancient times, organisms of any species on Earth have faced situations characterized by the presence of constraints. At the beginning they were related to the limitation of resources, such as food, now they involve many different areas, such as mechanics, engineering, economics, finance and biology. Modern problems appear to be very distant from those posed by nature millennia ago, yet many optimization methods take advantage of heuristics based on social behaviour of certain animals, such as bats \cite{bats}, bees \cite{bees}, ants \cite{ants}, lions, wolves and dolphins \cite{group_hunting}.

Dealing with constraints is a challenging task that appears in a wide variety of contexts, such as sampling or optimization. In an attempt to solve this kind of problems, mathematicians have explored new fields of study and devised new tools, contributing to the overall progress of science. It is known, for instance, that Greek mathematicians Euclid and Hero of Alexandria already solved some optimization problems related to their geometrical studies \cite{euclid}\cite{hero}.

New methods to deal with constraints have been designed after the invention of computers and our capability to solve large scale problems is increased with the development of faster processors and, in recent times, graphics processing units. In particular, general-purpose computing on graphics cards has enabled the execution of algorithms that were once considered computationally infeasible, widening research horizons.

The availability of larger amounts of data generated via Internet has improved the effectiveness of machine learning algorithms, especially those involving the training of artificial neural networks. Nowadays, these architectures often represent the state of the art for many different tasks, even achieving superhuman results in some of them \cite{888}.

The goal of this work is to describe a novel approach to instil prior knowledge in the form of constraints in a deep generative model. Multiple experiments on synthetic data sets have been executed to measure the impact of several factors on the final performance of the proposed model. Different design choices involving both the architectures and the training procedure have been explored, providing useful insights on a task that is not extensively analysed yet in literature.

Our results are encouraging since they show that prior information can effectively guide the generative model at approximating the data distribution more closely than its constraints-unaware counterpart. Further research is thus justified because the proposed model can be useful in many other contexts dealing with constraints. For instance, due to its characteristics, it can be used to perform rejection sampling efficiently.

Contents are organised as follows: chapter \ref{ch:background} provides background information on artificial neural networks and constrained problems; chapter \ref{ch:can} describes the new deep generative model and how it can solve known issues related to constrained problems; chapter \ref{ch:experiments_and_results} presents the conducted experiments and their results; chapter \ref{ch:conclusions} reports the conclusions drawn and future work ideas.
